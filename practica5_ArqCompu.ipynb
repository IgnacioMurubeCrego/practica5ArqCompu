{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN0aEKsDpfkZYJdVXpfRZH9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IgnacioMurubeCrego/practica5ArqCompu/blob/main/practica5_ArqCompu.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4Ik5PfN7NpK",
        "outputId": "8a39267b-8a70-4ae4-bd6d-23380eca4da3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2023 NVIDIA Corporation\n",
            "Built on Tue_Aug_15_22:02:13_PDT_2023\n",
            "Cuda compilation tools, release 12.2, V12.2.140\n",
            "Build cuda_12.2.r12.2/compiler.33191640_0\n"
          ]
        }
      ],
      "source": [
        "!nvcc --version"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/andreinechaev/nvcc4jupyter.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sKtYgStn8WX_",
        "outputId": "261f8218-448c-4395-d799-aa32f8a0997b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/andreinechaev/nvcc4jupyter.git\n",
            "  Cloning https://github.com/andreinechaev/nvcc4jupyter.git to /tmp/pip-req-build-sl12sro2\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/andreinechaev/nvcc4jupyter.git /tmp/pip-req-build-sl12sro2\n",
            "  Resolved https://github.com/andreinechaev/nvcc4jupyter.git to commit 28f872a2f99a1b201bcd0db14fdbc5a496b9bfd7\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: nvcc4jupyter\n",
            "  Building wheel for nvcc4jupyter (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nvcc4jupyter: filename=nvcc4jupyter-1.2.1-py3-none-any.whl size=10742 sha256=fdb30b442a03a2b978d94f242f4f3839c3b517111d4c5bcc31496f40b5aacbba\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-wt61d08c/wheels/a8/b9/18/23f8ef71ceb0f63297dd1903aedd067e6243a68ea756d6feea\n",
            "Successfully built nvcc4jupyter\n",
            "Installing collected packages: nvcc4jupyter\n",
            "Successfully installed nvcc4jupyter-1.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext nvcc4jupyter"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0zRQwV19LpR",
        "outputId": "d1804e98-cf20-4a25-865f-d06c41e6c658"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected platform \"Colab\". Running its setup...\n",
            "Source files will be saved in \"/tmp/tmpl9xwgq25\".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%cuda\n",
        "#include <stdio.h>\n",
        "\n",
        "__global__ void hello(){\n",
        "    printf(\"Hello World from block: %u, thread: %u\\n\", blockIdx.x, threadIdx.x);\n",
        "}\n",
        "\n",
        "int main(){\n",
        "    hello<<<2, 3>>>();\n",
        "    cudaDeviceSynchronize();\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DAdlAQYf9qLh",
        "outputId": "d1a064e2-2282-4069-9718-4c35ac78f07d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello World from block: 0, thread: 0\n",
            "Hello World from block: 0, thread: 1\n",
            "Hello World from block: 0, thread: 2\n",
            "Hello World from block: 1, thread: 0\n",
            "Hello World from block: 1, thread: 1\n",
            "Hello World from block: 1, thread: 2\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "wEgXVxh2EELn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ejemplo B :  duplicar un valor con CUDA\n"
      ],
      "metadata": {
        "id": "eVrZZDdhCJOt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%cuda\n",
        "#include <iostream>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "// DEFINICIÓN KERNEL CUDA\n",
        "\n",
        "__global__ void duplicarValor(int *valor_device)\n",
        "\n",
        "{\n",
        "  printf(\"Block [2%u], Thread [2%u]: valor_device=%d\\n\", blockIdx.x, threadIdx.x, *valor_device);\n",
        "  *valor_device *= 2;\n",
        "}\n",
        "\n",
        "// FUNCIÓN DEL HOST\n",
        "\n",
        "int main()\n",
        "{\n",
        "\n",
        "  // 1 - INICIALIZAR DATOS DEL HOST\n",
        "\n",
        "  int valor_host = 5;\n",
        "\n",
        "  // valor inicial en el host\n",
        "\n",
        "  // 2 - DECLARAR Y ASIGNAR MEMORIA PARA EL DEVICE\n",
        "\n",
        "  int *valor_device; // puntero al device\n",
        "\n",
        "  cudaMalloc((void **)&valor_device, sizeof(int)); // asignar memoria para el device\n",
        "\n",
        "  // 3 - TRANSFERIR DATOS DEL HOST AL DEVICE\n",
        "\n",
        "  cudaMemcpy(valor_device, &valor_host, sizeof(int), cudaMemcpyHostToDevice); // transferir el valor del host al device\n",
        "\n",
        "  // 4 - EJECUTAR EN UNO O MÁS KERNELS\n",
        "\n",
        "  duplicarValor<<<1, 2>>>(valor_device); // lanzar el kernel\n",
        "\n",
        "  cudaDeviceSynchronize(); // espera hasta que el kernel termine\n",
        "\n",
        "  // 5 - TRANSFERIR DATOS DEL DEVICE AL HOST\n",
        "\n",
        "  cudaMemcpy(&valor_host, valor_device, sizeof(int), cudaMemcpyDeviceToHost); // transferir el valor del device al host\n",
        "\n",
        "  // 6 - LIBERAR RECURSOS\n",
        "\n",
        "  cudaFree(valor_device); // liberar memoria del device\n",
        "\n",
        "  // MOSTRAR RESULTADO\n",
        "\n",
        "  std::cout << \"Resultado: \" << valor_host << std::endl;\n",
        "\n",
        "  return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Uz1xNMmCH5v",
        "outputId": "44e7521f-26e8-4d65-fef4-0a2a679d6f2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Block [20], Thread [20]: valor_device=5\n",
            "Block [20], Thread [21]: valor_device=5\n",
            "Resultado: 10\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ejemplo C : suma de vectores con CUDA\n"
      ],
      "metadata": {
        "id": "jB9IAicrEG1l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%cuda\n",
        "#include <iostream>\n",
        "#include <cuda_runtime.h>\n",
        "#define T_VEC 100\n",
        "\n",
        "//DEFINICIÓN KERNEL CUDA\n",
        "\n",
        "__global__ void sumaVectores(int* v1_device, int* v2_device, int*suma_device) {\n",
        "  printf(\"Índice de hilo dentro del bloque: %d\\n\", threadIdx.x);\n",
        "  printf(\"Índice del bloque: %d\\n\", blockIdx.x);\n",
        "  printf(\"Tamaño del bloque (número de hilos): %d\\n\", blockDim.x);\n",
        "  for (int i = 0; i < T_VEC; i++) {\n",
        "    suma_device[i] = v1_device[i] + v2_device[i];\n",
        "  }\n",
        "}\n",
        "\n",
        "//FUNCIÓN DEL HOST\n",
        "\n",
        "int main() {\n",
        "\n",
        "  //1 - INICIALIZAR DATOS DEL HOST\n",
        "\n",
        "  int v1_host[T_VEC], v2_host[T_VEC], suma_host[T_VEC];\n",
        "  for (int i = 0; i < T_VEC; i++) {\n",
        "    v1_host[i] = 1;\n",
        "    v2_host[i] = 2;\n",
        "  }\n",
        "\n",
        "  //2 - DECLARAR Y ASIGNAR MEMORIA PARA EL DEVICE\n",
        "\n",
        "  int *v1_device, *v2_device, *suma_device; //punteros al device\n",
        "  cudaMalloc((void**)&v1_device, sizeof(int) * T_VEC); //asignar memoria para el device\n",
        "  cudaMalloc((void**)&v2_device, sizeof(int) * T_VEC);\n",
        "  cudaMalloc((void**)&suma_device, sizeof(int) * T_VEC);\n",
        "\n",
        "  //3 - TRANSFERIR DATOS DEL HOST AL DEVICE\n",
        "\n",
        "  cudaMemcpy(v1_device, v1_host, sizeof(int) * T_VEC,cudaMemcpyHostToDevice); //transferir el valor del host al device\n",
        "  cudaMemcpy(v2_device, v2_host, sizeof(int) * T_VEC,cudaMemcpyHostToDevice);\n",
        "  //cudaMemcpy(suma_device, suma_host, sizeof(int) * T_VEC,cudaMemcpyHostToDevice); //Inutil\n",
        "\n",
        "  //4 - EJECUTAR EN UNO O MÁS KERNELS\n",
        "\n",
        "  sumaVectores<<<1, 1>>>(v1_device, v2_device, suma_device);\n",
        "  //lanzar el kernel\n",
        "  cudaDeviceSynchronize(); //espera hasta que el kernel termine\n",
        "\n",
        "  //5 - TRANSFERIR DATOS DEL DEVICE AL HOST\n",
        "\n",
        "  cudaMemcpy(suma_host, suma_device, sizeof(int) * T_VEC,\n",
        "  cudaMemcpyDeviceToHost); //transferir el valor del device al host\n",
        "\n",
        "  //6 - LIBERAR RECURSOS\n",
        "  cudaFree(v1_device); //liberar memoria del device\n",
        "  cudaFree(v2_device);\n",
        "  cudaFree(suma_device);\n",
        "\n",
        "  //MOSTRAR RESULTADO\n",
        "  for (int i = 0; i < T_VEC; i++) {\n",
        "    std::cout << suma_host[i] << \" \";\n",
        "  }\n",
        "  std::cout << \"\\n\";\n",
        " return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hgy32yb-ENMm",
        "outputId": "5d22cf5d-1a13-40c2-a9d3-7f1e239a905d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Índice de hilo dentro del bloque: 0\n",
            "Índice del bloque: 0\n",
            "Tamaño del bloque (número de hilos): 1\n",
            "3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Ejercicio 1 (4 puntos)\n",
        "Modificar el ejemplo de suma de vectores con CUDA para que se ejecute en un bloque de\n",
        "25 hilos, dónde cada uno se encargue de sumar una parte de los vectores."
      ],
      "metadata": {
        "id": "qXT_T5BrGqLu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%cuda\n",
        "#include <iostream>\n",
        "#include <cuda_runtime.h>\n",
        "#define T_VEC 100\n",
        "\n",
        "//DEFINICIÓN KERNEL CUDA\n",
        "\n",
        "__global__ void sumaVectores(int* v1_device, int* v2_device, int*suma_device) {\n",
        "  // printf(\"Índice de hilo dentro del bloque: %d\\n\", threadIdx.x);\n",
        "  // printf(\"Índice del bloque: %d\\n\", blockIdx.x);\n",
        "  // printf(\"Tamaño del bloque (número de hilos): %d\\n\", blockDim.x);\n",
        "  int threadInBlockIndex = threadIdx.x + blockIdx.x * blockDim.x;\n",
        "\n",
        "  // Cada hilo suma un cuarto del vector\n",
        "    for (int i = threadInBlockIndex; i < T_VEC; i += blockDim.x * gridDim.x) {\n",
        "        suma_device[i] = v1_device[i] + v2_device[i];\n",
        "    }\n",
        "}\n",
        "\n",
        "//FUNCIÓN DEL HOST\n",
        "\n",
        "int main() {\n",
        "\n",
        "  //1 - INICIALIZAR DATOS DEL HOST\n",
        "\n",
        "  int v1_host[T_VEC], v2_host[T_VEC], suma_host[T_VEC];\n",
        "  for (int i = 0; i < T_VEC; i++) {\n",
        "    v1_host[i] = 1;\n",
        "    v2_host[i] = 2;\n",
        "  }\n",
        "\n",
        "  //2 - DECLARAR Y ASIGNAR MEMORIA PARA EL DEVICE\n",
        "\n",
        "  int *v1_device, *v2_device, *suma_device; //punteros al device\n",
        "  cudaMalloc((void**)&v1_device, sizeof(int) * T_VEC); //asignar memoria para el device\n",
        "  cudaMalloc((void**)&v2_device, sizeof(int) * T_VEC);\n",
        "  cudaMalloc((void**)&suma_device, sizeof(int) * T_VEC);\n",
        "\n",
        "  //3 - TRANSFERIR DATOS DEL HOST AL DEVICE\n",
        "\n",
        "  cudaMemcpy(v1_device, v1_host, sizeof(int) * T_VEC,cudaMemcpyHostToDevice); //transferir el valor del host al device\n",
        "  cudaMemcpy(v2_device, v2_host, sizeof(int) * T_VEC,cudaMemcpyHostToDevice);\n",
        "  //cudaMemcpy(suma_device, suma_host, sizeof(int) * T_VEC,cudaMemcpyHostToDevice); //Inutil\n",
        "\n",
        "  //4 - EJECUTAR EN UNO O MÁS KERNELS\n",
        "\n",
        "  sumaVectores<<<1, 25>>>(v1_device, v2_device, suma_device);\n",
        "  //lanzar el kernel\n",
        "  cudaDeviceSynchronize(); //espera hasta que el kernel termine\n",
        "\n",
        "  //5 - TRANSFERIR DATOS DEL DEVICE AL HOST\n",
        "\n",
        "  cudaMemcpy(suma_host, suma_device, sizeof(int) * T_VEC,\n",
        "  cudaMemcpyDeviceToHost); //transferir el valor del device al host\n",
        "\n",
        "  //6 - LIBERAR RECURSOS\n",
        "  cudaFree(v1_device); //liberar memoria del device\n",
        "  cudaFree(v2_device);\n",
        "  cudaFree(suma_device);\n",
        "\n",
        "  //MOSTRAR RESULTADO\n",
        "  for (int i = 0; i < T_VEC; i++) {\n",
        "    std::cout << suma_host[i] << \" \";\n",
        "  }\n",
        "  std::cout << \"\\n\";\n",
        " return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a8a1977-7929-4bfe-ec10-2cbfbb49ffd8",
        "id": "dDhRmBCMGz5q"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Ejercicio 2 (5 puntos)\n",
        "Desarrollar un programa donde se realice la suma de cinco vectores de 200 valores con\n",
        "CUDA de manera paralela. Se debe ejecutar con 10 bloques de 20 hilos."
      ],
      "metadata": {
        "id": "AaS3nC---g1-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%cuda\n",
        "#include <iostream>\n",
        "#include <cuda_runtime.h>\n",
        "#define VECTOR_SIZE 200\n",
        "#define THREADS_PER_BLOCK 20\n",
        "#define BLOCKS 10\n",
        "\n",
        "// Kernel para sumar cinco vectores\n",
        "__global__ void sumaVectores(int* v1_device, int* v2_device, int* v3_device, int* v4_device, int* v5_device, int* suma_device) {\n",
        "    int tid = threadIdx.x + blockIdx.x * blockDim.x; // Índice global del hilo\n",
        "\n",
        "    if (tid < VECTOR_SIZE) { // Verificar que el hilo está dentro del rango válido\n",
        "        suma_device[tid] = v1_device[tid] + v2_device[tid] + v3_device[tid] + v4_device[tid] + v5_device[tid];\n",
        "    }\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    // 1. Inicializar los vectores en el host\n",
        "    int v1_host[VECTOR_SIZE], v2_host[VECTOR_SIZE], v3_host[VECTOR_SIZE], v4_host[VECTOR_SIZE], v5_host[VECTOR_SIZE], suma_host[VECTOR_SIZE];\n",
        "\n",
        "    for (int i = 0; i < VECTOR_SIZE; i++) {\n",
        "        v1_host[i] = 1;\n",
        "        v2_host[i] = 2;\n",
        "        v3_host[i] = 3;\n",
        "        v4_host[i] = 4;\n",
        "        v5_host[i] = 5;\n",
        "    }\n",
        "\n",
        "    // 2. Declarar y asignar memoria para el device\n",
        "    int *v1_device, *v2_device, *v3_device, *v4_device, *v5_device, *suma_device;\n",
        "    cudaMalloc((void**)&v1_device, sizeof(int) * VECTOR_SIZE);\n",
        "    cudaMalloc((void**)&v2_device, sizeof(int) * VECTOR_SIZE);\n",
        "    cudaMalloc((void**)&v3_device, sizeof(int) * VECTOR_SIZE);\n",
        "    cudaMalloc((void**)&v4_device, sizeof(int) * VECTOR_SIZE);\n",
        "    cudaMalloc((void**)&v5_device, sizeof(int) * VECTOR_SIZE);\n",
        "    cudaMalloc((void**)&suma_device, sizeof(int) * VECTOR_SIZE);\n",
        "\n",
        "    // 3. Transferir datos del host al device\n",
        "    cudaMemcpy(v1_device, v1_host, sizeof(int) * VECTOR_SIZE, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(v2_device, v2_host, sizeof(int) * VECTOR_SIZE, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(v3_device, v3_host, sizeof(int) * VECTOR_SIZE, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(v4_device, v4_host, sizeof(int) * VECTOR_SIZE, cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(v5_device, v5_host, sizeof(int) * VECTOR_SIZE, cudaMemcpyHostToDevice);\n",
        "\n",
        "    // 4. Ejecutar el kernel\n",
        "    sumaVectores<<<BLOCKS, THREADS_PER_BLOCK>>>(v1_device, v2_device, v3_device, v4_device, v5_device, suma_device);\n",
        "    cudaDeviceSynchronize(); // Esperar a que termine el kernel\n",
        "\n",
        "    // 5. Transferir los resultados de vuelta al host\n",
        "    cudaMemcpy(suma_host, suma_device, sizeof(int) * VECTOR_SIZE, cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // 6. Liberar memoria del device\n",
        "    cudaFree(v1_device);\n",
        "    cudaFree(v2_device);\n",
        "    cudaFree(v3_device);\n",
        "    cudaFree(v4_device);\n",
        "    cudaFree(v5_device);\n",
        "    cudaFree(suma_device);\n",
        "\n",
        "    // 7. Mostrar los resultados\n",
        "    std::cout << \"Resultado de la suma de los cinco vectores:\" << std::endl;\n",
        "    for (int i = 0; i < VECTOR_SIZE; i++) {\n",
        "        std::cout << suma_host[i] << \" \";\n",
        "    }\n",
        "    std::cout << \"\\n\";\n",
        "\n",
        "    return 0;\n",
        "}\n"
      ],
      "metadata": {
        "id": "oVz6XNQb-gZ5",
        "outputId": "0e411715-14bd-4dc0-c818-e7460eb0de9d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resultado de la suma de los cinco vectores:\n",
            "15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 15 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejercicio 3 (1 puntos)\n",
        "###Explicar en qué casos se usarían más o menos número de bloques, hilos por bloque, etc.\n",
        "### Por ejemplo, cuando interesa utilizar solo un bloque con muchos hilos, muchos bloques con pocos hilos, o un punto intermedio.\n"
      ],
      "metadata": {
        "id": "PbmP5g5-DmMJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La elección entre usar más bloques, más hilos por bloque o un punto intermedio entre ambas depende de varios factores, como la arquitectura de la GPU, el tamaño del problema, y el tipo de operación que se quiere aplicar.\n",
        "\n",
        "En general, optar por utilizar un bloque con muchos hilos es preferible para procesar cantidades pequeñas de datos, especialmente cuando los datos están relacioados entre sí, o deben ser compartidos con el resto de hilos. Es decir, cuando la comunicación entre hilos es necesaria un bloque con muchos hilos favorece la facilidad de sincronización y acceso entre hilos.\n",
        "\n",
        "Luego, en el caso de trabajar con grandes cantidades de datos es conveniente optar por un mayor número de bloques con menos hilos dado que esto facilita la escalabilidad para resolver problemas grandes y permite distribuir el trabajo en múltiples bloques donde se aprovecha mejor el paralelismo masivo de la GPU."
      ],
      "metadata": {
        "id": "Ka6222-kDwyL"
      }
    }
  ]
}